MSE<-SSE/df.error
F0<-MSR/MSE
VP<-pf(F0,k,df.error,lower.tail=F)
result<-data.frame(SumSq=c(SSR,SSE),Df=c(k,df.error),MeanSq=c(MSR,MSE),
F0=c(round(F0,digits=3),' '),P.value=c(format(VP,scientific = TRUE,digits=3),' '),
row.names =c("Modelo","Error"))
cat("Tabla ANOVA Modelo de Regresión","\n")
result
}
#FIN DE FUNCIONES PERSONALIZADAS PARA OBTENER RESULTADOS DE RLM
#INSTALACIÓN Y CARGA DE PAQUETES NECESARIOS
install.packages(c("car","leaps"))
require(car)
require(leaps)
install.packages("leaps")
require(car)
require(leaps)
base40=read.table(file.choose(),header=T,sep=",")
attach(base40)
#MATRIZ DE DISPERSIÓN
plot(base40)
#AJUSTE DEL MODELO, TABLA ANOVA Y PARÁMETROS ESTIMADOS
modelo<-lm(Y~.,base40)
miAnova(modelo)
summary(modelo)
base40$ID<-NULL
View(base40)
#INICIO DE FUNCIONES PERSONALIZADAS PARA OBTENER RESULTADOS DE RLM
#TABLA DE TODAS LAS REGRESIONES POSIBLES
allregtable<-function(modeloreg,respuesta){
t1<-summary(regsubsets(model.matrix(modeloreg)[,-1],respuesta,
nbest=2^(modeloreg$rank-1)-1,really.big=T))
t2<-as.vector(apply(t1$which[,-1],1,sum))
t3<-apply(t1$which[,-1],1,
function(x) as.character(paste(colnames(model.matrix(modeloreg)[,-1])[x],collapse=" ")))
results<-data.frame(NoOfVars=t2,R2=round(t1$rsq,4),adjR2=round(t1$adjr2,4),
SSE=round(t1$rss,5),Cp=round(t1$cp,4),Variables.in.model=t3)
#,MSE=round(t1$rss/(nrow(model.matrix(modeloreg)[,-1])-(NoOfVars+1)),5)
results
}
#PARA OBTENER LA ANOVA DEL MODELO DE RLM CREAMOS LA SIGUIENTE FUNCIÓN
miAnova<-function(modeloreg){
SSq<-unlist(anova(modeloreg)["Sum Sq"])
k<-length(SSq)-1
SSR<-sum(SSq[1:k])
SSE<-SSq[(k+1)]
MSR<-SSR/k
df.error<-unlist(anova(modeloreg)["Df"])[k+1]
MSE<-SSE/df.error
F0<-MSR/MSE
VP<-pf(F0,k,df.error,lower.tail=F)
result<-data.frame(SumSq=c(SSR,SSE),Df=c(k,df.error),MeanSq=c(MSR,MSE),
F0=c(round(F0,digits=3),' '),P.value=c(format(VP,scientific = TRUE,digits=3),' '),
row.names =c("Modelo","Error"))
cat("Tabla ANOVA Modelo de Regresión","\n")
result
}
#FIN DE FUNCIONES PERSONALIZADAS PARA OBTENER RESULTADOS DE RLM
#INSTALACIÓN Y CARGA DE PAQUETES NECESARIOS
install.packages(c("car","leaps"))
require(car)
require(leaps)
attach(base40)
#MATRIZ DE DISPERSIÓN
plot(base40)
#AJUSTE DEL MODELO, TABLA ANOVA Y PARÁMETROS ESTIMADOS
modelo<-lm(Y~.,base40)
miAnova(modelo)
summary(modelo)
install.packages(c("car", "leaps"))
#INICIO DE FUNCIONES PERSONALIZADAS PARA OBTENER RESULTADOS DE RLM
#TABLA DE TODAS LAS REGRESIONES POSIBLES
allregtable<-function(modeloreg,respuesta){
t1<-summary(regsubsets(model.matrix(modeloreg)[,-1],respuesta,
nbest=2^(modeloreg$rank-1)-1,really.big=T))
t2<-as.vector(apply(t1$which[,-1],1,sum))
t3<-apply(t1$which[,-1],1,
function(x) as.character(paste(colnames(model.matrix(modeloreg)[,-1])[x],collapse=" ")))
results<-data.frame(NoOfVars=t2,R2=round(t1$rsq,4),adjR2=round(t1$adjr2,4),
SSE=round(t1$rss,5),Cp=round(t1$cp,4),Variables.in.model=t3)
#,MSE=round(t1$rss/(nrow(model.matrix(modeloreg)[,-1])-(NoOfVars+1)),5)
results
}
allregtable(modelo,Y)
}
allregtable(modelo,Y)
#INICIO DE FUNCIONES PERSONALIZADAS PARA OBTENER RESULTADOS DE RLM
#TABLA DE TODAS LAS REGRESIONES POSIBLES
allregtable<-function(modeloreg,respuesta){
t1<-summary(regsubsets(model.matrix(modeloreg)[,-1],respuesta,
nbest=2^(modeloreg$rank-1)-1,really.big=T))
t2<-as.vector(apply(t1$which[,-1],1,sum))
t3<-apply(t1$which[,-1],1,
function(x) as.character(paste(colnames(model.matrix(modeloreg)[,-1])[x],collapse=" ")))
results<-data.frame(NoOfVars=t2,R2=round(t1$rsq,4),adjR2=round(t1$adjr2,4),
SSE=round(t1$rss,5),Cp=round(t1$cp,4),Variables.in.model=t3)
#,MSE=round(t1$rss/(nrow(model.matrix(modeloreg)[,-1])-(NoOfVars+1)),5)
results
}
modelo
Y
allregtable(modelo,Y)
true
TRUE
TRUE+TRUE+FALSE+TRUE
TRUE+TRUE
TRUE
T
!T
number_vector<-c(0,0,1,5,299)
as.logical(number_vector)
x<-5
y<-25
x+y
my_variable<-"I like apples"
class(my_variable)
a<-T
a
b<-T+F
b
a+b
startup_data<-read.csv('crunchbase_monthly_export.csv',header=T,stringAsFactors=F)
startup_data<-read.csv('crunchbase_monthly_export.csv',header=T,stringsAsFactors=F)
View(startup_data)
help(read.csv)
head(startup_data)
tail(startup_data)
nrows(startup_data)
nrow(startup_data)
nol(startup_data)
ncol(startup_data)
colnames(startup_data)
tail(startup_data)
ncol(startup_data)
colnames(startup_data)
help(mean)
my_vector<-c(2,14,10,15,3)
my_vector
my_vector*2
my_vector+my_vector
my_vector+my_vector
my_vector
my_vector[1]
my_vector[1:3]
my_vector[-2]
my_vector
my_vector>2
my_vector[my_vector>2]
my_matrix<-matrix(1:6,ncol=3,nrow=2,byrow=TRUE)
my_matrix<-matrix(1:6,ncol=3,nrow=2,byrow=TRUE)
my_matrix
my_matrix<-matrix(1:6,ncol=3,nrow=2,byrow=FALSE)
my_matrix
my_matrix[1,]
my_matrix[1,:]
y_matrix[1,]
my_matrix[1,]
my_matrix
my_matrix[,1:2]
is.data.frame(startup_data)
is.matrix(startup_data)
startup_data$name
startup_data$name[1:50]
startup_data$name[1:50,]
young_startups<-startup_data$founded_year>2012
young_startups
young_startups[1:10]
startup_data$foundedyear[1:10]
young_startups[1:10]
startup_data$founded_year[1:10]
sum(young_startups)
sum(young_startups,na.rm=TRUE)
startup_data$name
startup_data$name[1]
colnames(startup_data)
head(startup_data)
startup_data$state_code=="NY"
sum(startup_data$state_code=="NY")
startup_data$state_code=="NY"
head(startup_data)
startup_data[,1:50]
startup_data[1:50,]
sum(startup_data[1:50,]$funding_rounds)
hist(startup_data$fundind_rounds,main="Series A crunch",xlab="Fuding Rounds",ylab="# of startups",xlim=c(1,15))
hist(startup_data$fundind_rounds,main="Series A crunch",xlab="Fuding Rounds",ylab="# of startups",xlim=c(1,15))
startup_data$fundind_rounds,
startup_data$fundind_rounds
hist(startup_data$funding_rounds,main="Series A crunch",xlab="Fuding Rounds",ylab="# of startups",xlim=c(1,15))
par(bg="#36394A",col="white",fg="white",col.axis="white",col.lab="white",col.main="white",family="Verdana")
hist(startup_data$funding_rounds,main="Series A crunch",xlab="Fuding Rounds",ylab="# of startups",xlim=c(1,15))
par(bg="#36394A",col="white",fg="white",col.axis="white",col.lab="white",col.main="white",family="Verdana")
help(par)
getwd()
class_data<-read.csv('classes.csv',header=T,stringAsFactors=F)
class_data<-read.csv('classes.csv',header=T,stringsAsFactors=F)
head(class_data)
levels(class_data)
level(class_data)
str(class_data)
head(class_data)
View(Data)
View(class_data)
match(class_data$A,class_data$B,nomatch=0)
class_data$A[14]
class_data$A[23]
class_data$B[29]
class_data$B[20]
matches<-class_data$A %in% class_data$B
matches
class_data$A[matches]
class_data$A[14]
class_data$A[23]
class_data$B[29]
class_data$B[20]
quiz_class_data<-('classes_test.csv',header=T, stringsAsFactors=F)
quiz_class_data<-read.csv('classes_test.csv',header=T, stringsAsFactors=F)
head(quiz_class_data)
quiz_class_data
quiz_class_data(1,1)
quiz_class_data[1,1]
quiz_class_data[1,:]
quiz_class_data[1,]
quiz_class_data$A="Trina"
quiz_class_data$A
quiz_class_data<-read.csv('classes_test.csv',header=T, stringsAsFactors=F)
quiz_class_data$A=="Trina"
quiz_class_data(quiz_class_data$A=="Trina")
quiz_class_data[quiz_class_data$A=="Trina"]
quiz_class_data[quiz_class_data$A=="Trina",]
match(quiz_class_data$A,quiz_class_data$B,nomatch=0)
quiz_class_data$A[31]
quiz_class_data$B[12]
quiz_class_data$A[12]
quiz_class_data$B[31]
quiz_class_data$A[12]
quiz_class_data$B[35]
quiz_class_data$A[35]
quiz_class_data$B[12]
quiz_class_data$A[36]
quiz_class_data$B[12]
library(caret)
#setwd("C:/Users/Alfonso/Desktop/JOM/Practical_Machine_Learning")
setwd("C:/Users/JosePortatil/Dropbox/Data Science/Practical_Machine_Learning")
training<-read.csv("pml-training.csv",header=T)
testing<-read.csv("pml-testing.csv",header=T)
colnames(training)
head(traning)
head(training)
summary(training)
training<-read.csv("pml-training.csv",header=T,na.strings = "")
testing<-read.csv("pml-testing.csv",header=T,na.strings = "")
summary(training)
colSums(is.na(testing))>2
which(colSums(is.na(testing))>2)
which(colSums(is.na(testing))>19000)
is.na(testing)
View(testing)
which(colSums(is.na(training))>19000)
colSums(is.na(training))>19000
which(colSums(is.na(training))>19000)
delete_columns<-which(colSums(is.na(training))>19000)
delete_columns
training<-training[,-c(delete_columns)]
training$X<-NULL
training$user_name<-NULL
training$raw_timestamp_part_1<-NULL
training$raw_timestamp_part_2<-NULL
training$cvtd_timestamp<-NULL
training$new_window<-NULL
training$num_window<-NULL
training<-read.csv("pml-training.csv",header=T,na.strings = "")
testing<-read.csv("pml-testing.csv",header=T,na.strings = "")
training$X<-NULL
training$user_name<-NULL
training$raw_timestamp_part_1<-NULL
training$raw_timestamp_part_2<-NULL
training$cvtd_timestamp<-NULL
training$new_window<-NULL
training$num_window<-NULL
delete_columns<-which(colSums(is.na(training))>500)
delete_columns
delete_columns<-which(colSums(is.na(training))>100)
delete_columns<-which(colSums(is.na(training))>10)
delete_columns<-which(colSums(is.na(training))>1)
training<-training[,-c(delete_columns)]
summary(training)
training$var_pitch_forearm
str(training$var_pitch_forearm)
View(training)
colnames(training)
str(training$var_pitch_forearm)
training<-read.csv("pml-training.csv",header=T,na.strings = "")
testing<-read.csv("pml-testing.csv",header=T,na.strings = "")
#Drop Not relevant Columns.
training$X<-NULL
training$user_name<-NULL
training$raw_timestamp_part_1<-NULL
training$raw_timestamp_part_2<-NULL
training$cvtd_timestamp<-NULL
training$new_window<-NULL
training$num_window<-NULL
colnames(training)
training[, -c(153)] <- sapply(training[,-c(153)], as.numeric)
str(training$var_pitch_forearm)
summary(training)
delete_columns<-which(colSums(is.na(training))>1)
delete_columns
training$var_pitch_forearm
training<-read.csv("pml-training.csv",header=T,na.strings = "")
testing<-read.csv("pml-testing.csv",header=T,na.strings = "")
training$var_pitch_forearm
tail(training$var_pitch_forearm)
training[, -c(153)] <- sapply(training[,-c(153)], as.numeric)
tail(training$var_pitch_forearm)
setwd("C:/Users/JosePortatil/Dropbox/Data Science/Practical_Machine_Learning")
training<-read.csv("pml-training.csv",header=T,na.strings = "")
testing<-read.csv("pml-testing.csv",header=T,na.strings = "")
training$X<-NULL
training$user_name<-NULL
training$raw_timestamp_part_1<-NULL
training$raw_timestamp_part_2<-NULL
training$cvtd_timestamp<-NULL
training$new_window<-NULL
training$num_window<-NULL
tail(training$var_pitch_forearm)
training[, -c(153)] <- sapply(training[,-c(153)], as.character)
training[, -c(153)] <- sapply(training[,-c(153)], as.numeric)
tail(training$var_pitch_forearm)
delete_columns<-which(colSums(is.na(training))>1)
delete_columns<-which(colSums(is.na(training))>100)
delete_columns<-which(colSums(is.na(training))>1000)
delete_columns<-which(colSums(is.na(training))>19000)
training<-training[,-c(delete_columns)]
tail(training$var_pitch_forearm)
training$var_pitch_forearm
summary(training)
View(testing)
#Drop Not relevant Columns.
testing$X<-NULL
testing$user_name<-NULL
testing$raw_timestamp_part_1<-NULL
testing$raw_timestamp_part_2<-NULL
testing$cvtd_timestamp<-NULL
testing$new_window<-NULL
testing$num_window<-NULL
testing<-testing[,-c(delete_columns)]
colnames(training)
testing<-read.csv("pml-testing.csv",header=T,na.strings = "")
testing$X<-NULL
testing$user_name<-NULL
testing$raw_timestamp_part_1<-NULL
testing$raw_timestamp_part_2<-NULL
testing$cvtd_timestamp<-NULL
testing$new_window<-NULL
testing$num_window<-NULL
testing[, -c(153)] <- sapply(testing[,-c(153)], as.character)
testing[, -c(153)] <- sapply(testing[,-c(153)], as.numeric)
testing<-testing[,-c(delete_columns)]
View(testing)
tetsing$problem_id<-NULL
testing$problem_id<-NULL
training$problem_id<-NULL
training$problem_id<-NULL
colnames(training)
View(testing)
testing<-read.csv("pml-testing.csv",header=T,na.strings = "")
colnames(testing)
testing$X<-NULL
testing$user_name<-NULL
testing$raw_timestamp_part_1<-NULL
testing$raw_timestamp_part_2<-NULL
testing$cvtd_timestamp<-NULL
testing$new_window<-NULL
testing$num_window<-NULL
testing$problem_id<-NULL
colnames(training)
colnames(testing)
testing[, -c(153)] <- sapply(testing[,-c(153)], as.character)
testing[, -c(153)] <- sapply(testing[,-c(153)], as.numeric)
testing<-testing[,-c(delete_columns)]
View(training)
library(caret)
#Create Training and test Sets
data(iris)
inTrain<-createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training<-iris[inTrain,]
testing<-iris[-inTrain,]
modFit<-train(Species~.,data=training,method="rf",prox=TRUE)
modFit
View(training)
pred<-predict(modFit,testing)
testing$predRight<-pred==testing$Species
table(pred,testing$Species)
modFit
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="cv", number=10)
train_control
model <- train(Species~., data=iris, trControl=train_control, method="nb")
View(iris)
predictions <- predict(model, iris[,1:4])
confusionMatrix(predictions, iris$Species)
train_control <- trainControl(method="cv", number=10)
library(caret)
#setwd("C:/Users/Alfonso/Desktop/JOM/Practical_Machine_Learning")
setwd("C:/Users/JosePortatil/Dropbox/Data Science/Practical_Machine_Learning")
training<-read.csv("pml-training.csv",header=T,na.strings = "")
testing<-read.csv("pml-testing.csv",header=T,na.strings = "")
#####################################
##########      TIPS       ##########
#####################################
#I didn't treat this as a time series.
#Random Forest gives very accurate result in this case.
#You do need to do some initial cleanup and also run the preProcess for rf to run efficiently.
#Try Boosting
#(For a tidy data set of this course project, the columns is about 50~60.)
#Take a smaller size of training set (subset of training data).
#(I did the course project with only 1000~2000 records in training set and got approximately 90% precision.)
#Set ntree parameter when using randomForest(). (ntree around 100 is enough.)
#Get Rid of X as one of the predictors.
####################################################
colnames(training)
summary(training)
###################################
#######  CLEANING PART ############
###################################
#***** Clean Treaning Data.
#FIRST PHASE
#Drop Not relevant Columns.
training$X<-NULL
training$user_name<-NULL
training$raw_timestamp_part_1<-NULL
training$raw_timestamp_part_2<-NULL
training$cvtd_timestamp<-NULL
training$new_window<-NULL
training$num_window<-NULL
training$problem_id<-NULL
#Phase 2
#Convert COlumn Variables to Numeric [First convert to Character then to Numeric]
training[, -c(153)] <- sapply(training[,-c(153)], as.character)
training[, -c(153)] <- sapply(training[,-c(153)], as.numeric)
#Drop columns with lots of NA values
delete_columns<-which(colSums(is.na(training))>19000)
training<-training[,-c(delete_columns)]
###CLEAN TESTING DATA
#FIRST PHASE
#Drop Not relevant Columns.
testing$X<-NULL
testing$user_name<-NULL
testing$raw_timestamp_part_1<-NULL
testing$raw_timestamp_part_2<-NULL
testing$cvtd_timestamp<-NULL
testing$new_window<-NULL
testing$num_window<-NULL
testing$problem_id<-NULL
#Convert COlumn Variables to Numeric [First convert to Character then to Numeric]
testing[, -c(153)] <- sapply(testing[,-c(153)], as.character)
testing[, -c(153)] <- sapply(testing[,-c(153)], as.numeric)
#Drop columns with lots of NA values
testing<-testing[,-c(delete_columns)]
colnames(training)
predictions <- predict(model, training[,1:52])
model <- train(Species~., data=training, trControl=train_control, method="rf")
model <- train(classe~., data=training, trControl=train_control, method="rf")
train_control <- trainControl(method="cv", number=10)
# train the model
model <- train(classe~., data=training, trControl=train_control, method="rf")
training_sample <- training[sample(1:nrow(mydata), 2000,
replace=FALSE),]
training_sample <- training[sample(1:nrow(training), 2000,
replace=FALSE),]
View(training_sample)
training_sample <- training[sample(1:nrow(training), 2000,
replace=FALSE),]
# define training control
train_control <- trainControl(method="cv", number=10)
# train the model
model <- train(classe~., data=training_sample, trControl=train_control, method="rf")
save.image("C:/Users/JosePortatil/Dropbox/Data Science/Practical_Machine_Learning/jj.RData")
predictions <- predict(model, training_sample[,1:52])
confusionMatrix(predictions, training_sample$classe)
predictions
training_sample[,1:52]
confusionMatrix(predictions, training_sample$classe)
View(training_sample)
colnames(training_sample)
# define training control
train_control <- trainControl(method="cv", number=10)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
model
test_sample <- training[sample(1:nrow(training), 2000,
replace=FALSE),]
View(test_sample)
predictions <- predict(model, test_sample[,1:52])
# summarize results
confusionMatrix(predictions, test_sample$classe)
View(testing)
Final_predictions <- predict(model, testing)
Final_predictions
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(Final_predictions)
Final_predictions <- predict(model, testing)
Final_predictions
